{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Trained DNN\n",
    "\n",
    "Once the DNN models are trained, we implement validation and inference stages. We do this for one trained model at a time, so select the most optimal one for application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chainer\n",
    "from chainer import configuration\n",
    "from chainer.dataset import convert\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import serializers\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "\n",
    "import import_ipynb\n",
    "import multi_mod_compact\n",
    "from multi_mod_compact import MLP1, MLP2, MLP3\n",
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chosen architecture\n",
    "dnn_final = MLP1()\n",
    "dnn_final = L.Classifier(dnn_final)\n",
    "dnn_final.to_device(device)\n",
    "\n",
    "# Load the parameters of the trained DNN\n",
    "directory = 'Generalization'\n",
    "serializers.load_npz(os.path.join(directory,'MLP1.model'), dnn_final)\n",
    "dnn_final = dnn_final.predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Stage\n",
    "\n",
    "We construct the confusion matrix of the trained DNN. For this task, we use a separate validation dataset. This is generated the same way that the testing dataset was generated. Make sure to accomplish that first before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the validation datasets\n",
    "\n",
    "dataset_dir = 'Datasets'\n",
    "T00A = pickle.load(open(os.path.join(dataset_dir,'T00Ainputs_valid.pkl'),\"rb\"))\n",
    "T00A = cp.asarray(T00A, dtype=np.float32)\n",
    "T00B = pickle.load(open(os.path.join(dataset_dir,'T00Binputs_valid.pkl'),\"rb\"))\n",
    "T00B = cp.asarray(T00B, dtype=np.float32)\n",
    "P01 = pickle.load(open(os.path.join(dataset_dir,'P01inputs_valid.pkl'),\"rb\"))\n",
    "P01 = cp.asarray(P01, dtype=np.float32)\n",
    "P02 = pickle.load(open(os.path.join(dataset_dir,'P02inputs_valid.pkl'),\"rb\"))\n",
    "P02 = cp.asarray(P02, dtype=np.float32)\n",
    "P03 = pickle.load(open(os.path.join(dataset_dir,'P03inputs_valid.pkl'),\"rb\"))\n",
    "P03 = cp.asarray(P03, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the validation datasets into the trained DNN and obtain the predicted labels\n",
    "# We do this separately for each classification so that we know their proper labels\n",
    "\n",
    "P01_labels = []\n",
    "for n in range(len(P01)):\n",
    "    label = dnn_final(P01)[n].array.argmax()\n",
    "    P01_labels.append(label.tolist())\n",
    "    \n",
    "P02_labels = []\n",
    "for n in range(len(P02)):\n",
    "    label = dnn_final(P02)[n].array.argmax()\n",
    "    P02_labels.append(label.tolist())\n",
    "    \n",
    "P03_labels = []\n",
    "for n in range(len(P03)):\n",
    "    label = dnn_final(P03)[n].array.argmax()\n",
    "    P03_labels.append(label.tolist())\n",
    "    \n",
    "T00A_labels = []\n",
    "for n in range(len(T00A)):\n",
    "    label = dnn_final(T00A)[n].array.argmax()\n",
    "    T00A_labels.append(label.tolist())\n",
    "\n",
    "T00B_labels = []\n",
    "for n in range(len(T00B)):\n",
    "    label = dnn_final(T00B)[n].array.argmax()\n",
    "    T00B_labels.append(label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the confusion matrix\n",
    "# We count the number of data from each classification that was predicted as a particular label\n",
    "\n",
    "table_data = np.array([\n",
    "    [T00A_labels.count(0), T00B_labels.count(0), P01_labels.count(0), P02_labels.count(0), P03_labels.count(0)],\n",
    "    [T00A_labels.count(1), T00B_labels.count(1), P01_labels.count(1), P02_labels.count(1), P03_labels.count(1)],\n",
    "    [T00A_labels.count(2), T00B_labels.count(2), P01_labels.count(2), P02_labels.count(2), P03_labels.count(2)],\n",
    "    [T00A_labels.count(3), T00B_labels.count(3), P01_labels.count(3), P02_labels.count(3), P03_labels.count(3)]])\n",
    "\n",
    "table_col = ['T00A', 'T00B', 'P01', 'P02', 'P03']\n",
    "table_row = [' 0 ', ' 1 ', ' 2 ', ' 3 ']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(table_data, cmap='viridis')\n",
    "\n",
    "ax.set_xticks(np.arange(len(table_col)), labels=table_col, fontsize = 20)\n",
    "ax.set_yticks(np.arange(len(table_row)), labels=table_row, fontsize = 20)\n",
    "\n",
    "for i in range(len(table_col)):\n",
    "    for j in range(len(table_row)):\n",
    "        if (i==2 and j==1) or (i==3 and j==2) or (i==4 and j==3):\n",
    "            text = ax.text(i, j, table_data[j, i], ha=\"center\", va=\"center\", color=\"k\", fontsize = 18)\n",
    "        else:\n",
    "            text = ax.text(i, j, table_data[j, i], ha=\"center\", va=\"center\", color=\"w\", fontsize = 18)\n",
    "\n",
    "fig.text(0.5, 0.9, 'True Label', ha='center', fontsize=16)\n",
    "fig.text(0.04, 0.5, 'Predicted Label', va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Stage\n",
    "\n",
    "We perform inference by feeding experimental data into the trained DNN and obtaining its predictions. We use this to intrepret our exotic signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experimental data\n",
    "LHCb_data = np.loadtxt('LHCb_Data.csv', skiprows=1, delimiter=',')\n",
    "invmass_low = LHCb_data[:, 1]\n",
    "invmass_high = LHCb_data[:, 2]\n",
    "weighted_candidates = LHCb_data[:, 3]\n",
    "upper_err = LHCb_data[:, 4]\n",
    "\n",
    "# Define region of interest\n",
    "energy_low = 4200.0\n",
    "energy_high = 4350.0\n",
    "\n",
    "# Randomly generate energy points from experimental data\n",
    "def gen_Eaxis():\n",
    "    x_data = []\n",
    "    \n",
    "    for i in range(len(invmass)):\n",
    "        # We use a uniform distribution\n",
    "        x_val = random.uniform(invmass_low[i],invmass_high[i])\n",
    "        x_data.append(x_val)\n",
    "    \n",
    "    return np.array(x_data)\n",
    "\n",
    "# Randomly generate intensity points from experimental data\n",
    "def gen_Amp():\n",
    "    y_data = []\n",
    "\n",
    "    for i in range(len(weighted_candidates)):\n",
    "        # We use a normal distribution\n",
    "        y_val = np.random.normal(weighted_candidates[i],upper_err[i])\n",
    "        y_data.append(y_val)\n",
    "\n",
    "    return np.array(y_data)\n",
    "\n",
    "# Combine the two and cut to region of interest\n",
    "def gen_Exp():\n",
    "    x_data = gen_Eaxis()\n",
    "    y_data = gen_Amp()\n",
    "\n",
    "    y_data = y_data[x_data < energy_high]\n",
    "    x_data = x_data[x_data < energy_high]\n",
    "    y_data = y_data[x_data >= energy_low]\n",
    "    x_data = x_data[x_data >= energy_low]\n",
    "\n",
    "    return np.concatenate((x_data,y_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct N number of line shapes from the experimental data\n",
    "\n",
    "N = 3000\n",
    "Exp_data = []\n",
    "\n",
    "for i in range(N):\n",
    "    Exp = gen_Exp()\n",
    "    Exp_data.append(Exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now feed the constructed line shapes into the trained DNN and obtain predictions\n",
    "\n",
    "Exp_data = cp.asarray(Exp_data, dtype=np.float32)\n",
    "\n",
    "Exp_labels = []\n",
    "for n in range(len(Exp_data)):\n",
    "    label = dnn_final(Exp_data)[n].array.argmax()\n",
    "    Exp_labels.append(label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of obtained predictions per label\n",
    "\n",
    "table_data = [\n",
    "    [Exp_labels.count(0)], [Exp_labels.count(1)],\n",
    "    [Exp_labels.count(2)], [Exp_labels.count(3)], [3000]]\n",
    "\n",
    "table_col = ['$P_\\psi^N(4312)^+$']\n",
    "table_row = [' 0 ', ' 1 ', ' 2 ', ' 3 ', ' Total ']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "ax.set_axis_off() \n",
    "table = ax.table(cellText = table_data, rowLabels = table_row, colLabels = table_col, rowLoc ='right', loc ='upper left',\n",
    "                rowColours = ['lightsteelblue']*4+['silver'], colColours = ['lightsteelblue'])\n",
    "\n",
    "table.scale(1,1.5)\n",
    "\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
